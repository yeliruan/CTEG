import tensorflow as tf
from tensorflow.python.layers import core as layers_core
from config import Config
from util import *
from tensorflow.nn.rnn_cell import LSTMStateTuple
from tensorflow.python.ops.rnn import bidirectional_dynamic_rnn
from memory_wrapper import AttentionMemWrapper
from collections import defaultdict


class Generator:
    def __init__(self, config):
        # configuration
        self.embedding_size = config["embedding_size"]
        self.hidden_size = config["hidden_size"]
        self.max_len = config["max_len"]
        self.attention_size = config["attention_size"]
        self.learning_rate = config["learning_rate"]
        self.sos_token = config["start_token"]
        self.eos_token = config["eos_token"]
        self.batch_size = config["batch_size"]
        self.sequence_lengths = [self.max_len] * self.batch_size

        self.vocab_size = config["vocab_size"]
        self.vocab_dict = config["vocab_dict"]
        self.vocab_size = 50000 + 4  # GO EOS UNK PAD
        # len(self.vocab_dict)

        self.grad_norm = config["grad_norm"]
        self.topic_num = config["topic_num"]
        self.training_flag = config["is_training"]
        self.keep_prob = config["keep_prob"]
        self.norm_init = config["norm_init"]
        self.normal_std = config["normal_std"]
        self.pretrain_wv = config["pretrain_wv"]
        self.beam_width = config["beam_width"]
        self.mem_num = config["mem_num"]
        self.refers = None
        self.rand_uni_init = tf.random_uniform_initializer(-self.norm_init, self.norm_init,
                                                           seed=123)
        self.trunc_norm_init = tf.truncated_normal_initializer(stddev=self.normal_std)
        self.sm = SmoothingFunction()

    def build_placeholder(self):
        # placeholder
        self.topic_input = tf.placeholder(tf.int32, [self.batch_size, self.topic_num], name="topic_index")
        self.topic_len = tf.placeholder(tf.int32, [self.batch_size], name="topic_len")

        self.target_input = tf.placeholder(tf.int32, [self.batch_size, None], name="target_index")
        self.target_len = tf.placeholder(tf.int32, [self.batch_size], name="target_len")
        # self.target_mask = tf.placeholder(tf.float32, [self.batch_size, self.max_len], name="target_mask")

        # memory
        self.memory_idx = tf.placeholder(tf.int32, [self.batch_size, self.mem_num], name="memory_index")

        # token generated by generator
        # self.x = tf.placeholder(dtype=tf.int32, shape=[self.batch_size, self.max_len])
        # reward
        self.rewards = tf.placeholder(dtype=tf.float32, shape=[None, None])

        # rollout placeholders
        self.rollout_input_ids = tf.placeholder(dtype=tf.int32, shape=[None, None])
        self.rollout_input_length = tf.placeholder(dtype=tf.int32, shape=())
        self.rollout_input_lengths = tf.placeholder(dtype=tf.int32, shape=[None])
        self.rollout_next_id = tf.placeholder(dtype=tf.int32, shape=[None])

    def _add_encoder(self, encoder_inputs, seq_len):
        with tf.variable_scope('encoder'):
            cell_fw = tf.contrib.rnn.LSTMCell(self.hidden_size, initializer=self.rand_uni_init,
                                              state_is_tuple=True)

            cell_bw = tf.contrib.rnn.LSTMCell(self.hidden_size, initializer=self.rand_uni_init,
                                              state_is_tuple=True)
            outputs, states = bidirectional_dynamic_rnn(cell_fw, cell_bw, encoder_inputs,
                                                        sequence_length=seq_len,
                                                        swap_memory=True, dtype=tf.float32)
            # encoder_outputs, encoder_states = tf.nn.dynamic_rnn(cell_fw, encoder_inputs,
            #                                                     dtype=tf.float32, sequence_length=seq_len,
            #                                                     swap_memory=True)

            fw_outputs, bw_outputs = outputs
            c_fw, h_fw = states[0]
            c_bw, h_bw = states[1]

            encoder_c = c_fw + c_bw
            encoder_h = h_fw + h_bw
            # assemble to be a state tuple object
            encoder_states = LSTMStateTuple(c=encoder_c, h=encoder_h)
            encoder_outputs = fw_outputs + bw_outputs  # add outputs
            # print(outputs)
            # print(states)
            return encoder_outputs, encoder_states

    def build_graph(self, use_memory=True):
        print("building generator graph...")
        with tf.variable_scope("seq2seq"):
            with tf.variable_scope("embedding"):

                # shared by encoder and decoder
                # self.embedding = tf.get_variable('embedding', [self.vocab_size, self.embedding_size], dtype=tf.float32,
                #                                  trainable=True, initializer=self.rand_uni_init)
                # using pretrain word vector
                self.embedding = tf.get_variable('embedding', [self.vocab_size, self.embedding_size], dtype=tf.float32,
                                                 trainable=True, initializer=tf.constant_initializer(self.pretrain_wv))
            with tf.variable_scope("encoder"):
                topic_embedded = tf.nn.embedding_lookup(self.embedding, self.topic_input)
                self.memory = tf.nn.embedding_lookup(self.embedding, self.memory_idx)
                # encode topic to representation
                encoder_outputs, encoder_last_states = self._add_encoder(topic_embedded,
                                                                         self.topic_len)  # print(fw_st.shape)

            with tf.variable_scope("decoder"):
                def _get_cell(_num_units):

                    cell = tf.contrib.rnn.BasicLSTMCell(_num_units)

                    if self.training_flag:
                        cell = tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=self.keep_prob)

                    return cell

                # decoder_c, deocder_h = fw_st

                # single layer
                self.decoder_cell = _get_cell(self.hidden_size)
                self.initial_state = encoder_last_states

                if use_memory:
                    self.uu = tf.get_variable('update_U', [self.embedding_size, self.embedding_size],
                                              trainable=True, initializer=tf.random_normal_initializer)
                    self.uv = tf.get_variable('update_V', [self.embedding_size, self.embedding_size],
                                              trainable=True, initializer=tf.random_normal_initializer)
                    self.w = tf.get_variable('memory_matrix', [self.hidden_size, self.embedding_size],
                                             trainable=True, initializer=tf.random_normal_initializer)
                    self.b = tf.get_variable('memory_bias', [self.embedding_size],
                                             trainable=True, initializer=tf.zeros_initializer)

                self.decoder_input_embedded = tf.nn.embedding_lookup(self.embedding, self.target_input)
                self.output_layer = layers_core.Dense(self.vocab_size, use_bias=False)

                # self.decoder_cell = tf.contrib.seq2seq.AttentionWrapper(
                #     self.decoder, attention_mechanism, attention_layer_size=self.hidden_size / 2)

                # pre-train with targets #
                helper_pt = tf.contrib.seq2seq.TrainingHelper(
                    inputs=self.decoder_input_embedded,
                    sequence_length=self.sequence_lengths,
                    time_major=False,
                )
                attention_size = 128
                self.attn_query = layers_core.Dense(attention_size, use_bias=False)
                self.attn_memory = layers_core.Dense(attention_size, use_bias=False)
                self.attn_v = tf.get_variable('attention_v', [attention_size],
                                              trainable=True, initializer=tf.random_normal_initializer)

                if use_memory:
                    with tf.name_scope("train"):
                        training_cell = AttentionMemWrapper(self.decoder_cell, self.memory, encoder_outputs,
                                                            self.batch_size, self.embedding_size,
                                                            self.hidden_size, self.mem_num, self.uu,
                                                            self.uv, self.w, self.b,
                                                            query_layer=self.attn_query,
                                                            memory_layer=self.attn_memory,
                                                            attention_v=self.attn_v
                                                            )
                    decoder_pt = tf.contrib.seq2seq.BasicDecoder(
                        cell=training_cell,
                        helper=helper_pt,
                        initial_state=self.initial_state,
                        output_layer=self.output_layer
                    )
                else:
                    # no memory
                    decoder_pt = tf.contrib.seq2seq.BasicDecoder(
                        cell=self.decoder_cell,
                        helper=helper_pt,
                        initial_state=self.initial_state,
                        output_layer=self.output_layer
                    )

                outputs_pt, _final_state, sequence_lengths_pt = tf.contrib.seq2seq.dynamic_decode(
                    decoder=decoder_pt,
                    output_time_major=False,
                    maximum_iterations=self.max_len,
                    swap_memory=True,
                    impute_finished=True
                )

                self.logits_pt = outputs_pt.rnn_output
                self.g_predictions = tf.nn.softmax(self.logits_pt)

                masks = tf.sequence_mask(lengths=self.target_len,
                                         maxlen=self.max_len, dtype=tf.float32, name='masks')

                self.target_output = tf.placeholder(tf.int32, [None, None])

                self.pretrain_loss = tf.contrib.seq2seq.sequence_loss(
                    self.logits_pt,
                    self.target_output,
                    masks,
                    average_across_timesteps=True,
                    average_across_batch=True)

                self.global_step = tf.Variable(0, trainable=False)

                # gradient clipping
                optimizer = tf.train.AdamOptimizer(self.learning_rate)
                gradients, v = zip(*optimizer.compute_gradients(self.pretrain_loss))
                gradients, _ = tf.clip_by_global_norm(gradients, self.grad_norm)
                self.pretrain_updates = optimizer.apply_gradients(zip(gradients, v), global_step=self.global_step)
                # original loss
                # if batch size is too big, there is going to OOM
                # print("prediction: ", self.g_predictions.shape)
                # print(tf.one_hot(tf.to_int32(tf.reshape(self.target_input, [-1])), self.vocab_size, 1.0, 0.0).shape)

                # logits weights for calculating loss, mask
                self.target_weights = tf.placeholder(dtype=tf.float32, shape=[None, None])

                # origin SeqGAN loss not work well , worse after about 300 steps
                # self.rewards_loss = -tf.reduce_sum(
                #     tf.reduce_sum(
                #         tf.one_hot(tf.to_int32(tf.reshape(self.target_input, [-1])), self.vocab_size, 1.0, 0.0) *
                #         tf.log(tf.clip_by_value(tf.reshape(self.g_predictions, [-1, self.vocab_size]), 1e-20, 1.0)),
                #         axis=1
                #     ) * tf.reshape(self.rewards, [-1]) * tf.reshape(self.target_weights, [-1])  # may need a weight mask
                # )

                # penalty-based loss
                self.rewards_loss = tf.reduce_sum(
                    tf.reduce_sum(
                        tf.one_hot(tf.to_int32(tf.reshape(self.target_input, [-1])), self.vocab_size, 1.0, 0.0) *
                        tf.clip_by_value(
                            tf.reshape(self.g_predictions, [-1, self.vocab_size]), 1e-20, 1.0)
                        , 1) * tf.reshape(self.rewards, [-1]) * tf.reshape(self.target_weights, [-1])
                )

            optimizer_gan = tf.train.RMSPropOptimizer(self.learning_rate)
            gradients_gan, v_gan = zip(*optimizer_gan.compute_gradients(self.rewards_loss))
            gradients_gan, _gan = tf.clip_by_global_norm(gradients_gan, self.grad_norm)
            self.rewards_updates = optimizer_gan.apply_gradients(zip(gradients_gan, v_gan),
                                                                 global_step=self.global_step)

            ###################### train without targets ######################
            helper_o = tf.contrib.seq2seq.SampleEmbeddingHelper(
                self.embedding,
                tf.fill([self.batch_size], self.vocab_dict['<GO>']),
                end_token=self.vocab_dict['<EOS>']
            )

            if use_memory:
                sample_cell = AttentionMemWrapper(self.decoder_cell, self.memory, encoder_outputs,
                                                  self.batch_size, self.embedding_size,
                                                  self.hidden_size, self.mem_num, self.uu,
                                                  self.uv, self.w, self.b,
                                                  query_layer=self.attn_query,
                                                  memory_layer=self.attn_memory,
                                                  attention_v=self.attn_v
                                                  )
                decoder_o = tf.contrib.seq2seq.BasicDecoder(
                    cell=sample_cell,  # sample cell
                    helper=helper_o,
                    initial_state=self.initial_state,
                    output_layer=self.output_layer
                )
            else:
                # no memory
                decoder_o = tf.contrib.seq2seq.BasicDecoder(
                    cell=self.decoder_cell,
                    helper=helper_o,
                    initial_state=self.initial_state,
                    output_layer=self.output_layer
                )

            outputs_o, _final_state_o, sequence_lengths_o = tf.contrib.seq2seq.dynamic_decode(
                decoder=decoder_o,
                output_time_major=False,
                maximum_iterations=self.max_len,
                swap_memory=True,
            )

            self.out_lengths = sequence_lengths_o
            self.out_tokens = tf.unstack(outputs_o.sample_id, axis=0)

            # infer
            helper_i = tf.contrib.seq2seq.GreedyEmbeddingHelper(
                self.embedding,
                tf.fill([self.batch_size], self.vocab_dict['<GO>']),
                end_token=self.vocab_dict['<EOS>']
            )

            if use_memory:
                with tf.variable_scope("infer"):
                    infer_cell = AttentionMemWrapper(self.decoder_cell, self.memory, encoder_outputs,
                                                     self.batch_size, self.embedding_size,
                                                     self.hidden_size, self.mem_num, self.uu,
                                                     self.uv, self.w, self.b,
                                                     query_layer=self.attn_query,
                                                     memory_layer=self.attn_memory,
                                                     attention_v=self.attn_v
                                                     )
                decoder_i = tf.contrib.seq2seq.BasicDecoder(
                    cell=infer_cell,  # no memory
                    helper=helper_i,
                    initial_state=self.initial_state,
                    output_layer=self.output_layer
                )
            else:
                decoder_i = tf.contrib.seq2seq.BasicDecoder(
                    cell=self.decoder_cell,  # no memory
                    helper=helper_i,
                    initial_state=self.initial_state,
                    output_layer=self.output_layer
                )

            outputs_i, _final_state_i, sequence_lengths_i = tf.contrib.seq2seq.dynamic_decode(
                decoder=decoder_i,
                output_time_major=False,
                maximum_iterations=self.max_len,
                swap_memory=True,
                impute_finished=True
            )

            sample_id = outputs_i.sample_id

            self.infer_tokens = tf.unstack(sample_id, axis=0)
            # print("initial state : ", self.initial_state)

            # beam_inital_state = tf.tile()
            # beam search infer

            # decoder_beam = seq2seq.BeamSearchDecoder(
            #     cell=self.decoder_cell,
            #     embedding=self.embedding,
            #     start_tokens=tf.fill([self.batch_size], self.vocab_dict['<GO>']),
            #     end_token=self.vocab_dict['<EOS>'],
            #     initial_state=tf.contrib.seq2seq.tile_batch(encoder_last_states, multiplier=self.beam_width),
            #     beam_width=self.beam_width,
            #     output_layer=self.output_layer
            # )
            #
            # outputs_beam, _final_state_beam, sequence_lengths_beam = tf.contrib.seq2seq.dynamic_decode(
            #     decoder=decoder_beam,
            #     output_time_major=False,
            #     maximum_iterations=self.max_len,
            #     swap_memory=True,
            #     impute_finished=False
            # )

            # # currently we don't use the beam search ...
            # self.beam_ids = outputs_beam.predicted_ids
            self.beam_ids = None

            # rollout
            rollout_inputs = tf.nn.embedding_lookup(self.embedding, self.rollout_input_ids)
            helper_ro = tf.contrib.seq2seq.TrainingHelper(
                rollout_inputs,
                self.rollout_input_lengths
            )
            if use_memory:
                with tf.variable_scope("rollout"):
                    rollout_cell = AttentionMemWrapper(self.decoder_cell, self.memory, encoder_outputs,
                                                       self.batch_size, self.embedding_size,
                                                       self.hidden_size, self.mem_num, self.uu,
                                                       self.uv, self.w, self.b,
                                                       query_layer=self.attn_query,
                                                       memory_layer=self.attn_memory,
                                                       attention_v=self.attn_v
                                                       )
                    rollout_decoder = tf.contrib.seq2seq.BasicDecoder(
                        cell=rollout_cell,  # no memory
                        helper=helper_ro,
                        initial_state=self.initial_state,
                        output_layer=self.output_layer
                    )
            else:
                rollout_decoder = tf.contrib.seq2seq.BasicDecoder(
                    cell=self.decoder_cell,  # no memory
                    helper=helper_ro,
                    initial_state=self.initial_state,
                    output_layer=self.output_layer
                )

            _, final_state_ro, _ = tf.contrib.seq2seq.dynamic_decode(
                rollout_decoder,
                maximum_iterations=self.max_len,
                swap_memory=True
            )

            initial_state_MC = final_state_ro
            helper_MC = tf.contrib.seq2seq.SampleEmbeddingHelper(
                self.embedding,
                self.rollout_next_id,
                end_token=self.vocab_dict['<EOS>']
            )

            if use_memory:
                with tf.variable_scope("MC"):
                    mc_cell = AttentionMemWrapper(self.decoder_cell, self.memory, encoder_outputs,
                                                  self.batch_size, self.embedding_size,
                                                  self.hidden_size, self.mem_num, self.uu,
                                                  self.uv, self.w, self.b,
                                                  query_layer=self.attn_query,
                                                  memory_layer=self.attn_memory,
                                                  attention_v=self.attn_v
                                                  )

                rollout_decoder_MC = tf.contrib.seq2seq.BasicDecoder(
                    cell=mc_cell,
                    helper=helper_MC,
                    initial_state=initial_state_MC,
                    output_layer=self.output_layer
                )
            else:
                rollout_decoder_MC = tf.contrib.seq2seq.BasicDecoder(
                    cell=self.decoder_cell,
                    helper=helper_MC,
                    initial_state=initial_state_MC,
                    output_layer=self.output_layer
                )

            self.max_mc_length = tf.cast(self.max_len - self.rollout_input_length, tf.int32)
            decoder_output_MC, _, _ = tf.contrib.seq2seq.dynamic_decode(
                rollout_decoder_MC,
                output_time_major=False,
                maximum_iterations=self.max_mc_length,
                swap_memory=True
            )
            self.sample_id_MC = decoder_output_MC.sample_id

        print("generator graph built successfully")

    def inference(self, sess, feed_dict):
        return sess.run([self.out_tokens], feed_dict=feed_dict)

    def get_reward(self, sess, input_x, topic_input, topic_len, rollout_num, discriminator, source_label, memory):
        # x = self.pad_input_data(input_x, go_id)
        x, lengths_x = self._pad_input_data(input_x)
        input_x = self._padding(input_x, self.max_len)
        rewards = []
        batch_size = len(x)

        topic_idx_padded = self._pad_topic(topic_input)
        for i in range(rollout_num):
            # print(i)
            for given_num in range(1, self.max_len):

                rollout_next_id = []
                for _item in x:
                    rollout_next_id.append(_item[given_num])

                feed = {
                    self.topic_len: topic_len,
                    self.topic_input: topic_idx_padded,
                    self.memory_idx: memory,
                    self.rollout_input_ids: x,
                    self.rollout_input_length: given_num,
                    self.rollout_input_lengths: [given_num] * batch_size,
                    self.rollout_next_id: rollout_next_id
                }

                mc_samples = sess.run(self.sample_id_MC, feed)

                fix_samples = np.array(input_x)[:, 0: given_num]

                samples = np.concatenate((fix_samples, mc_samples), axis=1)

                samples = self._padding(samples, self.max_len)

                feed = {discriminator.input_x: samples, discriminator.input_y: source_label,
                        discriminator.dropout_keep_prob: 1.0}
                mlc_rewards = sess.run(discriminator.rewards_for_mlc, feed)
                # ypred = np.array([item[1] for item in ypred_for_auc])  # probability of being real
                if i == 0:
                    rewards.append(mlc_rewards)
                else:
                    rewards[given_num - 1] += mlc_rewards

            # the last token reward
            feed = {discriminator.input_x: input_x, discriminator.input_y: source_label,
                    discriminator.dropout_keep_prob: 1.0}
            mlc_rewards = sess.run(discriminator.rewards_for_mlc, feed)
            # probability of being real
            # ypred = np.array([item[1] for item in ypred_for_auc])

            # borrow idea from SentiGAN
            if i == 0:
                rewards.append(mlc_rewards)
            else:
                rewards[self.max_len - 1] += mlc_rewards

        rewards = np.transpose(np.array(rewards)) / (1.0 * rollout_num)  # batch_size x seq_length
        return rewards

    def generate_essay(self, sess, topic_idx, topic_len, memory=None, padding=False):
        if memory is None:
            raise Exception("must feed memory")
        if padding:
            topic_idx_padded = self._pad_topic(topic_idx)
            ret = sess.run(self.out_tokens, feed_dict={self.topic_input: topic_idx_padded,
                                                       self.topic_len: topic_len,
                                                       self.memory_idx: memory})
            return self._padding(ret, self.max_len)
        else:
            topic_idx_padded = self._pad_topic(topic_idx)

            return sess.run(self.out_tokens, feed_dict={self.topic_input: topic_idx_padded,
                                                        self.topic_len: topic_len,
                                                        self.memory_idx: memory})

    def evaluate_bleu(self, sess, topic_idx, topic_len, memory=None):
        if memory is None:
            raise Exception("must feed memory")
        topic_idx_padded = self._pad_topic(topic_idx)

        return sess.run(self.infer_tokens, feed_dict={self.topic_input: topic_idx_padded,
                                                      self.topic_len: topic_len,
                                                      self.memory_idx: memory})

    def get_beamsearch_ret(self, sess, topic_idx, topic_len, memory):
        topic_idx_padded = self._pad_topic(topic_idx)

        return sess.run(self.beam_ids, feed_dict={self.topic_input: topic_idx_padded,
                                                  self.topic_len: topic_len,
                                                  self.memory_idx: memory})

    def _padding(self, samples, max_len):
        batch_size = len(samples)
        samples_padded = np.zeros(shape=[batch_size, max_len], dtype=np.int32)  # == PAD
        for i, seq in enumerate(samples):
            for j, element in enumerate(seq):
                samples_padded[i, j] = element
        return samples_padded

    def run_pretrain_step(self, sess, batch_data):
        pre_train_fd = self._make_pretrain_feed_dict(batch_data)
        _, pretrain_loss = sess.run([self.pretrain_updates, self.pretrain_loss], feed_dict=pre_train_fd)
        return pretrain_loss

    def _make_pretrain_feed_dict(self, batch_data):
        topic_input, topic_len, target_input, target_len, train_mem = batch_data
        # , target_mask
        target_padded, _ = self._pad_input_data(target_input)
        target_output = self._pad_target_data(target_input)
        topic_padded = self._pad_topic(topic_input)
        return {self.topic_input: topic_padded,
                self.topic_len: topic_len,
                self.target_input: target_padded,
                self.target_output: target_output,
                self.target_len: target_len,
                self.memory_idx: train_mem
                }
        # self.target_mask: target_mask}

    def run_adversarial_step(self, sess, batch_data, rewards):
        adversarial_fd = self._make_adversarial_feed_dict(batch_data, rewards)
        rewards_updates, rewards_loss = sess.run([self.rewards_updates, self.rewards_loss],
                                                 feed_dict=adversarial_fd)
        return rewards_loss

    def _make_adversarial_feed_dict(self, batch_data, rewards):
        topic_input, topic_len, samples, memory_idx = batch_data
        topic_padded = self._pad_topic(topic_input)

        target_padded, target_length = self._pad_input_data(samples)
        target_output = self._pad_target_data(samples)
        reward_weights = self._get_weights(target_length)

        return {self.topic_input: topic_padded,
                self.topic_len: topic_len,
                self.memory_idx: memory_idx,
                self.target_input: target_padded,
                self.target_output: target_output,
                self.rewards: rewards,
                self.target_weights: reward_weights}

    def _get_weights(self, lengths):
        x_len = len(lengths)
        max_l = self.max_len
        ans = np.zeros((x_len, max_l))
        for ll in range(x_len):
            kk = lengths[ll] - 1
            for jj in range(kk):
                ans[ll][jj] = 1 / float(kk)
        return ans

    def _pad_input_data(self, x):
        max_l = self.max_len
        go_id = self.vocab_dict['<GO>']
        end_id = self.vocab_dict['<EOS>']
        x_len = len(x)
        ans = np.zeros((x_len, max_l), dtype=int)
        ans_lengths = []
        for i in range(x_len):
            ans[i][0] = go_id
            jj = min(len(x[i]), self.max_len - 2)
            for j in range(jj):
                ans[i][j + 1] = x[i][j]
            ans[i][jj + 1] = end_id
            ans_lengths.append(jj + 2)
        return ans, ans_lengths

    def _pad_target_data(self, x):
        max_l = self.max_len
        end_id = self.vocab_dict['<EOS>']
        x_len = len(x)
        ans = np.zeros((x_len, max_l), dtype=int)
        for i in range(x_len):
            jj = min(len(x[i]), max_l - 1)
            for j in range(jj):
                ans[i][j] = x[i][j]
            ans[i][jj] = end_id
        return ans

    def _pad_topic(self, x):
        max_num = self.topic_num  # 5
        size = len(x)
        ans = np.zeros((size, max_num), dtype=int)
        for i in range(size):
            true_len = min(len(x[i]), max_num)
            for j in range(true_len):
                ans[i][j] = x[i][j]
        return ans

    @staticmethod
    def restore(sess, saver, path):
        saver.restore(sess, save_path=path)
        print("load model successfully")

    def save(self, sess, path, var_list=None, global_step=None):
        saver = tf.train.Saver(var_list)
        save_path = saver.save(sess, save_path=path, global_step=global_step)
        print("model saved at %s" % save_path)

    def evaluate(self, sess, dataloader, idx2word, get_ret=False):
        print("evaluating ...")
        dataloader.reset_pointer()
        test_samples = []
        test_target = []
        topic_list = []
        total_bleu = 0
        for t_n in range(dataloader.num_batch):
            topic_idx, topic_len, target_idx, _, source_label, test_mem = dataloader.next_batch()
            # samples, beam_samples = self.evaluate_bleu(sess, topic_idx, topic_len, memory=test_mem)
            # best_beams = beam_samples[:,:, 0]

            samples = self.evaluate_bleu(sess, topic_idx, topic_len, memory=test_mem)

            test_samples.extend(samples)
            test_target.extend(target_idx)
            topic_list.extend(topic_idx)
            # test_bleu, max_bleu, best_ret, best_target = calc_bleu2(test_samples, test_target)
            # total_bleu += test_bleu
        # build a list
        tp = [sorted(x) for x in topic_list]  # sort topic word
        tw = list(map(lambda x: "".join([idx2word[w] for w in x]), tp))
        if self.refers is None:
            print("building refers ....")
            multi_refers = defaultdict(list)

            for w, r in zip(tw, test_target):
                multi_refers[w].append(r)
            self.refers = multi_refers

        for w, h in zip(tw, test_samples):
            refers = self.refers[w]
            if len(refers) == 0:
                raise Exception("Error")
            total_bleu += sentence_bleu(refers, h, weights=(0, 1, 0, 0), smoothing_function=self.sm.method1)

        if not get_ret:
            return total_bleu / len(tw) * 100
        else:
            return total_bleu / len(tw) * 100, topic_list, test_target, test_samples
        # return total_bleu / dataloader.num_batch


if __name__ == '__main__':
    config = Config().generator_config_zhihu
    config["vocab_dict"] = np.load("./data/zhihu/word_dict_zhihu.npy").item()
    config["pretrain_wv"] = np.load("./data/zhihu/wv_tencent.npy")
    config["attention_size"] = 128
    G = Generator(config)
    G.build_placeholder()
    G.build_graph()
